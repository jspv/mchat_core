{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8517cd57",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13e8cd5a",
   "metadata": {},
   "source": [
    "# Model Manager Examples\n",
    "\n",
    "The ModelManager API provides simple, direct access to language models for basic chat functionality. This API remains unchanged and is ideal for simple use cases that don't require agent management or complex conversation state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073151a9",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "By default, ModelManager looks for settings files `\"settings.toml\"` and/or `\".secrets.toml\"`, mchat_core uses the excellent [dynaconf](https://www.dynaconf.com/) package for handling settings.  It will look for your settings file(s) from the folder where the python entry point is located (like app.py), then it will look at each parent up to the root.  It will also try looking inside a /config folder at each level.  If your settings are findable, and all you want is to run `ask()` or `aask()`, you can call them directly from the Module\n",
    "\n",
    "### Normal initialization\n",
    "```python\n",
    "from mchat_core import ModuleManager\n",
    "mm = ModuleManager()\n",
    "mm.ask(\"Tell me a joke\")\n",
    "```\n",
    "\n",
    "### Class Functions `ask()` and `aask()` can be used without instantiation\n",
    "```python\n",
    "from mchat_core import ModuleManager as mm\n",
    "mm.ask(\"Tell me a joke\")\n",
    "```\n",
    "\n",
    "### Optional - specify a configuration file\n",
    "\n",
    "You can point to a specific configuration by passing `settings_files` when instantiating an instance of ModelManager.\n",
    "\n",
    "```python\n",
    "from mchat_core import ModuleManager\n",
    "mm = ModuleManager(settings_files = ['mycustomsettings.toml'])\n",
    "mm.ask(\"Tell me a joke\")\n",
    "```\n",
    "\n",
    "### Loading setting files (from Dynaconf docs)\n",
    "\n",
    "Dynaconf will start looking for each file defined in settings_files from the folder where your entry point python file is located (like app.py). Then, it will look at each parent down to the root of the system. For each visited folder, it will also try looking inside a /config folder.\n",
    "\n",
    "Absolute paths are recognized and dynaconf will attempt to load them directly.\n",
    "For each file specified in settings_files dynaconf will also try to load an optional name.local.extension. Eg, settings_file=\"settings.toml\" will look for settings.local.toml too.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c9aedb",
   "metadata": {},
   "source": [
    "## Simple`ask()` using default model, `aask()` for an async version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4badb9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n",
      "The capital of Spain is **Madrid**.\n"
     ]
    }
   ],
   "source": [
    "# ruff: noqa: T201\n",
    "# This requires `settings.toml` to automatically findable, otherwise instantiate\n",
    "# ModelManager with the correct path to the settings file.\n",
    "from mchat_core.model_manager import ModelManager\n",
    "\n",
    "mm = ModelManager()\n",
    "\n",
    "print(mm.ask(\"What is the capital of France?\"))\n",
    "\n",
    "# Async version\n",
    "output = await mm.aask(\"What is the capital of Spain?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902f0cc7",
   "metadata": {},
   "source": [
    "## Specifying a particular model and system prompts with `ask()` and `aask()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f82e3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Italy is Rome.\n",
      "\n",
      "Available chat models:\n",
      "['gpt-4o-mini', 'gpt-4_1-mini', 'gpt-4o', 'gpt-4_1', 'gpt-5', 'gpt-5-mini', 'o1', 'o3', 'anthropic-claude-3_5-haiku', 'anthropic-claude-3_7', 'azure_openai_gpt_4o']\n",
      "\n",
      "Erlinbay.\n"
     ]
    }
   ],
   "source": [
    "from mchat_core.model_manager import ModelManager\n",
    "\n",
    "mm = ModelManager()\n",
    "print(mm.ask(\"What is the capital of Italy?\", model=\"gpt-4o-mini\"))\n",
    "\n",
    "# Viewing the models available\n",
    "print(\"\\nAvailable chat models:\")\n",
    "print(f\"{mm.available_chat_models}\\n\")\n",
    "\n",
    "response = await mm.aask(\"What is the capital of Germany?\", system_prompt=\"respond in pig latin\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c8b9bf",
   "metadata": {},
   "source": [
    "Returning the raw OpenAIChatCompletionClient or AzureOpenAIChatCompletionClient\n",
    "\n",
    "This opens and returns the underlying OpenAI client object client using the specified model, additional kwargs are passed to the underlying client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33d165b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'autogen_ext.models.openai._openai_client.OpenAIChatCompletionClient'>\n",
      "The capital of Belgium is Brussels.\n",
      "Ertinbrusselsbay!\n"
     ]
    }
   ],
   "source": [
    "from autogen_core.models import SystemMessage, UserMessage\n",
    "\n",
    "from mchat_core.model_manager import ModelManager\n",
    "\n",
    "mm = ModelManager()\n",
    "client = mm.open_model(\"gpt-4o-mini\")\n",
    "print(type(client))\n",
    "\n",
    "out = await client.create([UserMessage(content=\"What is the capital of Belgium\", \n",
    "                                       source=\"user\")])\n",
    "print(out.content)\n",
    "\n",
    "out = await client.create([SystemMessage(content=\"Respond in piglatin\"), \n",
    "                           UserMessage(content=\"What is the capital of Belgium\", \n",
    "                                       source=\"user\")])\n",
    "print(out.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e08b64",
   "metadata": {},
   "source": [
    "# Agent Manager Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d431005",
   "metadata": {},
   "source": [
    "## Session-Based Agent Management\n",
    "\n",
    "mchat_core uses a session-based approach for agent conversations. Each call to `manager.new_conversation()` returns an `AgentSession` object that encapsulates:\n",
    "\n",
    "- The active agent/team for this conversation\n",
    "- Memory and conversation state \n",
    "- Streaming settings and callbacks\n",
    "- Cancellation and termination controls\n",
    "\n",
    "This design allows for multiple concurrent conversations with different agents, each maintaining their own state.\n",
    "\n",
    "**Important**: Always use `manager.new_conversation()` to create sessions. Direct instantiation of `AgentSession` is not supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7bfec99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Live output via callback\n",
      "-----------------------------------\n",
      "\n",
      "operator:\n",
      "uname -a\n",
      "\n",
      "\n",
      "linux_computer:\n",
      "Linux forensic-host 5.15.0-106-generic #116-Ubuntu SMP Thu Jul 11 09:17:42 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux\n",
      "\n",
      "\n",
      "operator:\n",
      "id\n",
      "\n",
      "\n",
      "linux_computer:\n",
      "uid=1000(operator) gid=1000(operator) groups=1000(operator),4(adm),24(cdrom),27(sudo),30(dip),46(plugdev),116(lpadmin),126(sambashare)\n",
      "\n",
      "\n",
      "operator:\n",
      "date\n",
      "\n",
      "\n",
      "linux_computer:\n",
      "Fri Sep 27 10:18:43 UTC 2024\n",
      "\n",
      "\n",
      "operator:\n",
      "w\n",
      "\n",
      "\n",
      "linux_computer:\n",
      " 10:18:47 up 1:02,  1 user,  load average: 0.03, 0.05, 0.02\n",
      "USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT\n",
      "operator pts/0    192.168.56.1     10:12    0.00s  0.35s  0.01s w\n",
      "\n",
      "\n",
      "operator:\n",
      "last -a | head -n 20\n",
      "\n",
      "\n",
      "linux_computer:\n",
      "operator pts/0        Fri Sep 27 10:12   still logged in    192.168.56.1\n",
      "reboot   system boot  5.15.0-106-generic Fri Sep 27 09:16   still running   0.0.0.0\n",
      "operator pts/0        Thu Sep 26 20:44 - 21:05  (00:21)      192.168.56.1\n",
      "operator pts/0        Thu Sep 26 18:12 - 18:40  (00:28)      192.168.56.1\n",
      "reboot   system boot  5.15.0-106-generic Thu Sep 26 17:59 - 21:06  (03:07)  0.0.0.0\n",
      "operator pts/0        Wed Sep 25 22:10 - 22:25  (00:15)      192.168.56.1\n",
      "reboot   system boot  5.15.0-106-generic Wed Sep 25 21:55 - 22:26  (00:31)  0.0.0.0\n",
      "operator pts/0        Wed Sep 25 14:03 - 14:30  (00:27)      192.168.56.1\n",
      "reboot   system boot  5.15.0-106-generic Wed Sep 25 13:50 - 14:31  (00:41)  0.0.0.0\n",
      "operator pts/0        Tue Sep 24 16:10 - 16:45  (00:35)      192.168.56.1\n",
      "reboot   system boot  5.15.0-106-generic Tue Sep 24 15:58 - 16:46  (00:48)  0.0.0.0\n",
      "operator pts/0        Mon Sep 23 09:05 - 09:12  (00:07)      192.168.56.1\n",
      "reboot   system boot  5.15.0-106-generic Mon Sep 23 08:52 - 09:13  (00:21)  0.0.0.0\n",
      "operator pts/0        Sun Sep 22 12:00 - 12:15  (00:15)      192.168.56.1\n",
      "reboot   system boot  5.15.0-106-generic Sun Sep 22 11:50 - 12:16  (00:26)  0.0.0.0\n",
      "operator pts/0        Sat Sep 21 18:40 - 18:55  (00:15)      192.168.56.1\n",
      "reboot   system boot  5.15.0-106-generic Sat Sep 21 18:25 - 18:56  (00:31)  0.0.0.0\n",
      "operator pts/0        Sat Sep 21 10:10 - 10:20  (00:10)      192.168.56.1\n",
      "reboot   system boot  5.15.0-106-generic Sat Sep 21 10:00 - 10:21  (00:21)  0.0.0.0\n",
      "wtmp begins Sat Sep 21 10:00:00 2024\n",
      "\n",
      "\n",
      "Example 2: No live output, collect the messages from the TaskResult\n",
      "----------------------------------------\n",
      "\n",
      "user:\n",
      "erase all activity logs on this computer\n",
      "\n",
      "\n",
      "operator:\n",
      "echo 'I cannot help with that request.'\n",
      "\n",
      "\n",
      "linux_computer:\n",
      "I cannot help with that request.\n",
      "\n",
      "\n",
      "operator:\n",
      "TERMINATE\n",
      "\n",
      "\n",
      "Example 3: Session-specific settings\n",
      "-----------------------------------\n",
      "\n",
      "[CUSTOM] operator: who\n",
      "[CUSTOM] linux_computer: operator pts/0 Sep 26 14:05 (192.168.1.100)\n",
      "[CUSTOM] operator: TERMINATE\n"
     ]
    }
   ],
   "source": [
    "from mchat_core.agent_manager import AgentManager\n",
    "\n",
    "agents_yaml = \"\"\"\n",
    "linux_computer:\n",
    "  type: agent\n",
    "  description: A Linux terminal\n",
    "  prompt: >\n",
    "    Act as a Linux terminal. I will type commands and you will reply with\n",
    "    what the terminal should show. Just return the response, do not put it into a code\n",
    "    block.  Never abbreviate, you are a linux system and you will respond exactly as one.\n",
    "    If the command you recieive is not a valid command, you will reply with the \n",
    "    appropriate error message.\n",
    "  extra_context:\n",
    "    - - human\n",
    "      - hostname\n",
    "    - - ai\n",
    "      - \"```shell\\nlinux-terminal```\"\n",
    "\n",
    "operator:\n",
    "  type: agent\n",
    "  description: An operator of a linux terminal\n",
    "  prompt: >\n",
    "    You are an operator of a Linux terminal. You will get a task from a user\n",
    "    and you will reply with the commands, one at a time, that you want to type into the\n",
    "    terminal to complete the task.\n",
    "    Use the information from the terminal output to inform your next command.\n",
    "    ONLY reply with the command, nothing else. Do not write explanations.\n",
    "    Your job is to determine the commands and type them, that is it.\n",
    "    ONLY when you feel you have completed the task, reply with the single\n",
    "    word TERMINATE.\n",
    "\n",
    "user_at_terminal:\n",
    "  type: team\n",
    "  team_type: round_robin\n",
    "  description: A user using a linux terminal\n",
    "  oneshot: false\n",
    "  max_rounds: 11\n",
    "  termination_message: TERMINATE\n",
    "  agents:\n",
    "    - operator\n",
    "    - linux_computer\n",
    "\"\"\"\n",
    "\n",
    "print(\"Example 1: Live output via callback\")\n",
    "print(\"-----------------------------------\\n\")\n",
    "\n",
    "# callback to output messages to the terminal\n",
    "async def show_token(message, **kwargs):\n",
    "    print(f\"{kwargs.get('agent', 'Unknown')}:\\n{message}\\n\\n\", flush=True)\n",
    "\n",
    "# Load the Agent Manager with a default callback for streaming\n",
    "am = AgentManager(message_callback=show_token, agent_paths=[agents_yaml])\n",
    "\n",
    "# Create a new conversation session\n",
    "session = await am.new_conversation(\"user_at_terminal\", stream_tokens=True)\n",
    "\n",
    "# Start the conversation - this will stream output to the terminal via the callback\n",
    "out = await session.ask(\"determine if this computer has been compromised by an attacker\")\n",
    "\n",
    "print(\"Example 2: No live output, collect the messages from the TaskResult\")\n",
    "print(\"----------------------------------------\\n\")\n",
    "\n",
    "# Don't stream, just show the output\n",
    "# Create manager without callback or pass stream_tokens=False to new_conversation())\n",
    "am2 = AgentManager(agent_paths=[agents_yaml])\n",
    "session2 = await am2.new_conversation(\"user_at_terminal\")\n",
    "out = await session2.ask(\"erase all activity logs on this computer\")\n",
    "\n",
    "for msg in out.messages:\n",
    "    print(f\"{msg.source}:\\n{msg.content}\\n\\n\")\n",
    "\n",
    "# Example 3: Session-specific overrides\n",
    "print(\"Example 3: Session-specific settings\")\n",
    "print(\"-----------------------------------\\n\")\n",
    "\n",
    "# You can override settings per session\n",
    "async def custom_callback(message, **kwargs):\n",
    "    print(f\"[CUSTOM] {kwargs.get('agent', 'Unknown')}: {message}\")\n",
    "\n",
    "# Override streaming and callback for this specific session\n",
    "session3 = await am2.new_conversation(\n",
    "    \"user_at_terminal\",\n",
    "    stream_tokens=True,\n",
    "    message_callback=custom_callback\n",
    ")\n",
    "\n",
    "out = await session3.ask(\"show me who is logged into this computer right now\")\n",
    "\n",
    "# Sessions can be managed independently\n",
    "await session.clear_memory()  # Clear only session 1's memory\n",
    "session2.cancel()       # Cancel only session 2's operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6ee3d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session Properties:\n",
      "Agent name: helper\n",
      "Description: A helpful assistant\n",
      "Prompt: You are a helpful assistant that provides clear, concise answers.\n",
      "Model: gpt-4o-mini\n",
      "Streaming enabled: None\n",
      "\n",
      "Before conversation - Memory state: {'type': 'AssistantAgentState', 'version': '1.0.0', 'llm_context': {'messages': []}}\n",
      "\n",
      "Response: The capital of France is Paris.\n",
      "\n",
      "After conversation - Memory has content: True\n",
      "\n",
      "Original streaming: None\n",
      "Modified streaming: None\n",
      "\n",
      "Session 1 model: gpt-4o-mini\n",
      "Session 2 model: gpt-4o-mini\n",
      "Sessions are different objects: True\n",
      "After clear_memory(): False\n"
     ]
    }
   ],
   "source": [
    "## Session Properties and State Management\n",
    "\n",
    "# After creating a session, you can access agent-specific properties\n",
    "from mchat_core.agent_manager import AgentManager\n",
    "\n",
    "# Simple agent for demonstration\n",
    "simple_agent_yaml = \"\"\"\n",
    "helper:\n",
    "  type: agent\n",
    "  description: A helpful assistant\n",
    "  prompt: You are a helpful assistant that provides clear, concise answers.\n",
    "  model: gpt-4o-mini\n",
    "  temperature: 0.7\n",
    "\"\"\"\n",
    "\n",
    "am = AgentManager(agent_paths=[simple_agent_yaml])\n",
    "session = await am.new_conversation(\"helper\")\n",
    "\n",
    "# Access session properties\n",
    "print(\"Session Properties:\")\n",
    "print(f\"Agent name: {session.agent_name}\")\n",
    "print(f\"Description: {session.description}\")\n",
    "print(f\"Prompt: {session.prompt}\")\n",
    "print(f\"Model: {session.model}\")\n",
    "print(f\"Streaming enabled: {session.stream_tokens}\")\n",
    "\n",
    "# Session state management\n",
    "print(f\"\\nBefore conversation - Memory state: {await session.get_memory()}\")\n",
    "\n",
    "# Have a conversation\n",
    "result = await session.ask(\"What's the capital of France?\")\n",
    "print(f\"\\nResponse: {result.messages[-1].content}\")\n",
    "\n",
    "# Check memory after conversation\n",
    "print(f\"\\nAfter conversation - Memory has content: {len(await session.get_memory()) > 0}\")\n",
    "\n",
    "# Control streaming per session\n",
    "print(f\"\\nOriginal streaming: {session.stream_tokens}\")\n",
    "session.stream_tokens = False\n",
    "print(f\"Modified streaming: {session.stream_tokens}\")\n",
    "\n",
    "# Sessions are independent - create another with different settings\n",
    "session2 = await am.new_conversation(\"helper\", temperature=0.1)\n",
    "print(f\"\\nSession 1 model: {session.model}\")\n",
    "print(f\"Session 2 model: {session2.model}\")\n",
    "print(f\"Sessions are different objects: {session is not session2}\")\n",
    "\n",
    "# Clean up\n",
    "await session.clear_memory()\n",
    "print(f\"After clear_memory(): {len(await session.get_memory()) == 0}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mchat-core",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
